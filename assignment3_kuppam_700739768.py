# -*- coding: utf-8 -*-
"""Assignment3_kuppam_700739768.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-SE01_zefaXhsmqeGez87Jo0SMuyoGFu

# Question 1
1. (Titanic Dataset)
1. Find the correlation between ‘survived’ (target column) and ‘sex’ column for the Titanic use case in class.
a. Do you think we should keep this feature?
2. Do at least two visualizations to describe or show correlations.
3. Implement Naïve Bayes method using scikit-learn library and report the accuracy.
"""

#Import the required libraries
import pandas as pd
import seaborn as sns
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split, cross_validate

#Read the data file
#Getting the Data
df=pd.read_csv("train.csv")

#Data Exploration/Analysis
df.info()

#listed the features description
df.describe()

df.head()

survived = 'survived'
not_survived = 'not survived'
fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))
women = df[df['Sex']=='female']
men = df[df['Sex']=='male']
ax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)
ax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)
ax.legend()
ax.set_title('Female')
ax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)
ax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)
ax.legend()
_ = ax.set_title('Male')

"""It is clear that men, and to a lesser extent women, have a higher chance of surviving when they are between the ages of 18 and 30. Between the ages of 14 and 40, women have a greater probability of surviving.

Between the ages of 5 and 18, men have a very low chance of surviving, but women do not. Furthermore, newborns have a slightly higher chance of surviving than adults.

Hence This feature contribute to a survival rate 
"""

#correlating between ‘survived’ (target column) and ‘sex’ column for the Titanic use case
le = preprocessing.LabelEncoder()
df['Sex'] = le.fit_transform(df.Sex.values)
df['Survived'].corr(df['Sex'])

matrix = df.corr()
print(matrix)

#visualizations to describe or show correlations.
df.corr().style.background_gradient(cmap="Blues")

#visualizations to describe or show correlations.
sns.heatmap(matrix, annot=True, vmax=1, vmin=-1, center=0, cmap='viridis')
plt.show()

"""Now we will train Naive Machine Learning model"""

#Implementing Naïve Bayes method using scikit-learn library

train_raw = pd.read_csv('train.csv')
test_raw = pd.read_csv('test.csv')
df.columns.values
# Join data to analyse and process the set as one.
train_raw['train'] = 1
test_raw['train'] = 0
df = train_raw.append(test_raw, sort=False)

features = ['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex', 'SibSp']
target = 'Survived'

df = df[features + [target] + ['train']]

#we have to deal with 4 categorical features:
#Name, Sex, Ticket and Embarked transfrom one after another into numeric.

df['Sex'] = df['Sex'].replace(["female", "male"], [0, 1])
df['Embarked'] = df['Embarked'].replace(['S', 'C', 'Q'], [1, 2, 3])
train = df.query('train == 1')
test = df.query('train == 0')

# Commented out IPython magic to ensure Python compatibility.
import warnings
from scipy.stats.stats import pearsonr

# %matplotlib inline
# Suppress warnings
warnings.filterwarnings("ignore")

# Convert missing values from the train set.
train.dropna(axis=0, inplace=True)
labels = train[target].values

train.drop(['train', target, 'Pclass'], axis=1, inplace=True)
test.drop(['train', target, 'Pclass'], axis=1, inplace=True)

#Train and Test Split Data
X_train, X_val, Y_train, Y_val = train_test_split(train, labels, test_size=0.5, random_state=1)

#Classifier Naive Bayer Algorithm
classifier = GaussianNB()

classifier.fit(X_train, Y_train)
#Calculating the accuracy
y_pred = classifier.predict(X_val)

# Summary of the predictions made by the classifier
print(classification_report(Y_val, y_pred))
print(confusion_matrix(Y_val, y_pred))
# Accuracy score
from sklearn.metrics import accuracy_score
print('accuracy is',accuracy_score(Y_val, y_pred))

"""**Question 2**
1. Implement Naïve Bayes method using scikit-learn library.
a. Use the glass dataset available in Link also provided in your assignment.
b. Use train_test_split to create training and testing part.
2. Evaluate the model on testing part using score and
"""

# Get the data
glass=pd.read_csv("glass.csv")

#Display the data
glass.head()

#Visualization of data
glass.corr().style.background_gradient(cmap="RdBu")

#Visualization of data
sns.heatmap(matrix, annot=True, vmax=1, vmin=-1, center=0, cmap='cubehelix')
plt.show()

features = ['Rl', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe']
target = 'Type'


X_train, X_val, Y_train, Y_val = train_test_split(glass.iloc[:,:-1], glass.iloc[:,-1],test_size=0.2, random_state=1)

"""1. Implement Naïve Bayes method using scikit-learn library.
2. Use train_test_split to create training and testing part.
3. Evaluate the model on testing part using score and
"""

classifier = GaussianNB()

classifier.fit(X_train, Y_train)

y_pred = classifier.predict(X_val)

# Summary of the predictions made by the classifier
print(classification_report(Y_val, y_pred))
print(confusion_matrix(Y_val, y_pred))
# Accuracy score
from sklearn.metrics import accuracy_score
print('The accuracy of Naive Bayes Algo is',accuracy_score(Y_val, y_pred))

"""1. Implement linear SVM method using scikit library
2. Use train_test_split to create training and testing part.
3. Evaluate the model on testing part using score and
"""

from sklearn.svm import SVC, LinearSVC

classifier = LinearSVC()

classifier.fit(X_train, Y_train)


y_pred = classifier.predict(X_val)

# Summary of the predictions made by the classifier
print(classification_report(Y_val, y_pred))
print(confusion_matrix(Y_val, y_pred))
# Accuracy score
from sklearn.metrics import accuracy_score
print('accuracy is',accuracy_score(Y_val, y_pred))

"""By seeing the above results, with Naive Bayes’s accuracy with 25.52 % and SVM has an accuracy of 62.79. On comparing the on two models, SVM is performing better than the Naive Bayes. Naive Bayes is a classifier and will therefore perform better with categorical data. Navie Bayes will consider every feature as an individual feature and SVM tries to build a relationship between each feature."""